{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-24 14:52:51,806] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mImpossible d’exécuter le code ; la session a été supprimée. Essayez de redémarrer le noyau."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe Kernel s’est bloqué lors de l’exécution du code dans la cellule active ou une cellule précédente. Veuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. Cliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. Pour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()  # Write a config file\n",
    "os._exit(00)  # Restart the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([8, 1])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch \n",
    "\n",
    "class RandomIntDataset(Dataset):\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 10000\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": torch.randint(0, self.vocab_size, (1,))}\n",
    "\n",
    "def create_dataloader(vocab_size, batch_size=8):\n",
    "    return DataLoader(RandomIntDataset(vocab_size), batch_size=batch_size)\n",
    "\n",
    "dataloader = create_dataloader(32000)\n",
    "for batch in dataloader:\n",
    "    print({k: v.shape for k, v in batch.items()})\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-24 16:06:10,690] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    set_seed,\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"num_epochs\": 3,\n",
    "    \"steps_per_epoch\": 100,\n",
    "    \"validation_steps\": 50,\n",
    "    \"batch_size\": 8, # Actual batch size will this x 8\n",
    "    \"seed\": 42,\n",
    "    \"vocab_size\": 32000,\n",
    "}\n",
    "\n",
    "def training_loop(model):\n",
    "    \n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # To have only one message (and not 8) per logs of Transformers or Datasets, we set the logging verbosity\n",
    "    # to INFO for the main process only.\n",
    "    if accelerator.is_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "        \n",
    "    dataloader = create_dataloader(hyperparameters[\"vocab_size\"], hyperparameters[\"batch_size\"])\n",
    "    \n",
    "    set_seed(hyperparameters[\"seed\"])\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
    "    \n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n",
    "    # prepare method.\n",
    "    model, optimizer, dataloader = accelerator.prepare(\n",
    "        model, optimizer, dataloader\n",
    "    )\n",
    "    \n",
    "    num_epochs = hyperparameters[\"num_epochs\"]\n",
    "\n",
    "    # Instantiate learning rate scheduler after preparing the training dataloader as the prepare method\n",
    "    # may change its length.\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=hyperparameters[\"steps_per_epoch\"] * num_epochs,\n",
    "    )\n",
    "    progress_bar = tqdm(range(num_epochs * hyperparameters[\"steps_per_epoch\"]), disable=not accelerator.is_main_process)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        model.lm_head.requires_grad_(False)\n",
    "        model.model.requires_grad_(False)\n",
    "        model.auxiliary_outputs.requires_grad_(True)\n",
    "        batch = next(iter(dataloader))\n",
    "        for step in range(hyperparameters[\"steps_per_epoch\"]):\n",
    "            outputs = model(batch)\n",
    "            loss = outputs.loss\n",
    "            lm_head_logits = outputs.logits[-1]\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            batch = torch.cat((batch, torch.multinomial(torch.softmax(lm_head_logits[:, -1, :], dim=-1), 1)), dim=-1)\n",
    "            \n",
    "        model.eval()\n",
    "        batch = next(iter(dataloader))\n",
    "        eval_loss = 0\n",
    "        for step in range(hyperparameters[\"validation_steps\"]):\n",
    "            outputs = model(batch)\n",
    "            eval_loss += outputs.loss\n",
    "        loss = eval_loss / hyperparameters[\"validation_steps\"]\n",
    "        \n",
    "        accelerator.print(f\"Epoch {epoch} loss: {loss.item()}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BranchyLlama were not initialized from the model checkpoint at openlm-research/open_llama_3b_v2 and are newly initialized: ['auxiliary_outputs.1.weight', 'auxiliary_outputs.4.weight', 'auxiliary_outputs.24.weight', 'auxiliary_outputs.21.weight', 'auxiliary_outputs.23.weight', 'auxiliary_outputs.16.weight', 'auxiliary_outputs.17.weight', 'auxiliary_outputs.14.weight', 'auxiliary_outputs.6.weight', 'auxiliary_outputs.2.weight', 'auxiliary_outputs.19.weight', 'auxiliary_outputs.3.weight', 'auxiliary_outputs.11.weight', 'auxiliary_outputs.0.weight', 'auxiliary_outputs.10.weight', 'auxiliary_outputs.9.weight', 'auxiliary_outputs.7.weight', 'auxiliary_outputs.15.weight', 'auxiliary_outputs.12.weight', 'auxiliary_outputs.18.weight', 'auxiliary_outputs.22.weight', 'auxiliary_outputs.5.weight', 'auxiliary_outputs.25.weight', 'auxiliary_outputs.13.weight', 'auxiliary_outputs.20.weight', 'auxiliary_outputs.8.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from src.branchymodel import BranchyLlama\n",
    "\n",
    "branchyllamaconf = BranchyLlama.config_class.from_pretrained(\n",
    "    \"openlm-research/open_llama_3b_v2\"\n",
    ")\n",
    "branchyllamaconf.self_supervision = True\n",
    "\n",
    "model = BranchyLlama.from_pretrained(\n",
    "    \"openlm-research/open_llama_3b_v2\", config=branchyllamaconf\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA has been initialized before the `notebook_launcher` could create a forked subprocess. This likely stems from an outside import causing issues once the `notebook_launcher()` is called. Please review your imports and test them when running the `notebook_launcher()` to identify which one is problematic.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/launchers.py:137\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     start_processes(launcher, args\u001b[39m=\u001b[39;49margs, nprocs\u001b[39m=\u001b[39;49mnum_processes, start_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfork\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m ProcessRaisedException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py:197\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    198\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py:160\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    159\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m original_trace\n\u001b[0;32m--> 160\u001b[0m \u001b[39mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[39m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/launch.py\", line 535, in __call__\n    self.launcher(*args)\n  File \"/tmp/ipykernel_163258/3506208235.py\", line 26, in training_loop\n    accelerator = Accelerator()\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 358, in __init__\n    self.state = AcceleratorState(\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/state.py\", line 720, in __init__\n    PartialState(cpu, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/state.py\", line 198, in __init__\n    torch.cuda.set_device(self.device)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 367, in set_device\n    torch._C._cuda_setDevice(device)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 252, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maccelerate\u001b[39;00m \u001b[39mimport\u001b[39;00m notebook_launcher\n\u001b[0;32m----> 3\u001b[0m notebook_launcher(training_loop, (model,), num_processes\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/launchers.py:140\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[39mexcept\u001b[39;00m ProcessRaisedException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 140\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mCUDA has been initialized before the `notebook_launcher` could create a forked subprocess. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mThis likely stems from an outside import causing issues once the `notebook_launcher()` is called. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mPlease review your imports and test them when running the `notebook_launcher()` to identify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mwhich one is problematic.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m                 ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39m# No need for a distributed launch otherwise as it's either CPU, GPU or MPS.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[39mif\u001b[39;00m is_mps_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA has been initialized before the `notebook_launcher` could create a forked subprocess. This likely stems from an outside import causing issues once the `notebook_launcher()` is called. Please review your imports and test them when running the `notebook_launcher()` to identify which one is problematic."
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_loop, (model,), num_processes=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
